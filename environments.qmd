<!---
https://openlandmap.github.io/book/012-compendium.html
-->

# Environmental data {#sec-environmental_data}

## Land Cover and Land Use {#sec-land_cover}

Land coverage and use was touched in @sec-land_coverage, where the focus was mainly on OpenStreetMap data as a source. However there is more datasets available, based on satellite observations.

### ESA WorldCover {#sec-esa_worldcover}

European Space Agency (ESA) released a freely accessible global land coverage data sets. These data sets are based on Sentinel-1 and Sentinel-2 data and contains 11 land cover classes with overall accuracy over 75%. The first set was released in 2021, the second, with higher quality, in 2022 [@zanagaESAWorldCover102021; @zanagaESAWorldCover102022]. The data is accessible through AWS Open Data Registry, Zenodo and/or [Terrascope](https://terrascope.be/en). Terrascope provides the data as WM(T)S services as well, which can be used directly in QGIS:

* WMTS: https://services.terrascope.be/wmts/v2
* WMS: https://services.terrascope.be/wms/v2


```{r}
#| label: tbl-esa_world_cover_classes
#| tbl-cap: "ESA WorldCover classes definition and color codes for layers [@vandekerchoveWorldCoverProductUser2022]"
#| code-fold: true

worldcover_file <- "data/WorldCover_PUM_V2.0.pdf"
if(!file.exists(worldcover_file)) {
  worldcover_file <- "https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/docs/WorldCover_PUM_V2.0.pdf"  
}

t <- tabulizer::extract_tables(file = worldcover_file,
                               pages = 15,
                               method = "stream",
                               output = "data.frame")[[1]]

colnames(t) <- c("map_code", "land_cover_class", "lccs_code", "definition", "color_code_rgb")
t <- t[2:nrow(t),]
t$map_code <- as.numeric(t$map_code)

map_codes <- t$map_code[!is.na(t$map_code)]

merge_cels <- function(column = "", start = 10) {
  stop <- map_codes[which(map_codes == start)+1]
  a <- which(t$map_code == start)
  if(is.na(stop)) {
    b <- nrow(t)
  } else {
    b <- which(t$map_code == stop)-1
  }
  t[a:b, column] |>
    paste(collapse = " ") |>
    stringr::str_trim()
}

n <- t[1:11,] 

n$map_code <- map_codes

n <- n |>
  dplyr::rowwise() |>
  dplyr::mutate(land_cover_class = merge_cels(column = "land_cover_class", start = map_code),
                lccs_code = merge_cels(column = "lccs_code", start = map_code),
                definition = merge_cels(column = "definition", start = map_code),
                color_code_rgb = merge_cels(column = "color_code_rgb", start = map_code),
                color = rgb(red = strsplit(color_code_rgb[[1]], split = "[,]")[[1]][1],
                            green = strsplit(color_code_rgb[[1]], split = "[,]")[[1]][2],
                            blue = strsplit(color_code_rgb[[1]], split = "[,]")[[1]][3],
                            maxColorValue = 255)
  )


n[,1:5] |>
  kableExtra::kable(escape = FALSE,
                    col.names = c("Map code", "Land cover class",
                                  "LCCS code", "Definition", "Color code (RGB)")) |>
  kableExtra::kable_classic_2() |>
  kableExtra::column_spec(5, background = n$color, width = "12%")

```
The data can be accessed from [AWS](https://registry.opendata.aws/esa-worldcover-vito/) using `aws cli`. To list the content of the storage you can use:

    aws s3 ls --no-sign-request s3://esa-worldcover/
    
and to sync it with local filesystem:

    aws s3 sync --no-sign-request s3://esa-worldcover/v200/2021/map /local/path
    
The other option would be to mount it as a local file system using `s3fs`:

    s3fs esa-worldcover /local/path -o public_bucket=1
    
The data is available as TIFF raster tiles in a regular grid (EPSG:4326) with the ellipsoid WGS 1984 (Terrestrial radius=6378 km). The resolution of the grid is 1/12000$\deg$ corresponding to ~ 10 m at equator. The individual tile cover 3x3 degrees of longitude/latitude, and the filenames are in form:

    ESA_WorldCover_10m_2021_v200_N06E012_Map.tif
    
where `NxxExxx` (or `SxxWxxx`) corresponds to latitude (Nort or South) and longitude (West or East). The single file can be downloaded with:

    aws s3 cp --no-sign-request s3://esa-worldcover/v200/2021/map/ESA_WorldCover_10m_2021_v200_N51E015_Map.tif /local/file.tif

or from URL:

    https://esa-worldcover.s3.eu-central-1.amazonaws.com/v200/2021/map/ESA_WorldCover_10m_2021_v200_N51E015_Map.tif

The AWS buckets can be accessed with {aws.s3} package [@aws.s3] however rasters can be accessed directly by `rast()` function from {terra} package [@terra] thanks to underlying [GDAL Virtual File Systems](https://gdal.org/user/virtual_file_systems.html). First of all we can download FlatGeobuf file containing a layer with tile names and their corresponding polygons:

```{r}
#| label: esa_worldcover_flat_geobufer
#| message: false
#| output: false

esa_tiles <- sf::st_read("https://esa-worldcover.s3.eu-central-1.amazonaws.com/esa_worldcover_grid.fgb")
```

```{r}
#| label: esa_worldcover_flat_geobufer_list
#| echo: false

head(esa_tiles)
```

```{r}
#| label: fig-esa_worldcover_flat_geobufer
#| fig-cap: "Spatial coverage of WorlCover data"
#| echo: false

terra::plot(esa_tiles, main = "")
```

Having tile names (column `ll_tile` in `esa_tiles` sf data frame) we can easy find a requested tile(s) by filtering the data frame by our area of interest. Below an example for land cover classes for my village and surroundings. The first step is to find bounding box of the village (`getbb()` from {osmdata}):

```{r}
#| label: get_lubnow_bbox

l <- osmdata::getbb("LubnÃ³w, trzebnicki", format_out = "sf_polygon")
```

The next step is to subset our `esa_tiles` data frame by `l` to find the tile(s) which intersects with. It can be done by simple subseting:

```{r}
#| label: esa_tiles_subseting

b <- esa_tiles[l, ,]
tile_name <- b[[1, "ll_tile"]]
tile_name
```

or by `st_filter()` function

```{r}
#| label: esa_tiles_subseting_with_filter

tile_name <- esa_tiles |>
  sf::st_filter(l) |>
  sf::st_drop_geometry() |>
  as.character()
tile_name
```

In next step we have to get the proper raster, crop it to the village bounding box, (re)assign levels and table colors. Please note, the levels and colors corresponds to land cover classes and colors from @tbl-esa_world_cover_classes.

```{r}
#| label: esa_world_class_raster

terra::setGDALconfig("AWS_NO_SIGN_REQUEST=YES")
url <- paste0("s3://esa-worldcover/v200/2021/map/ESA_WorldCover_10m_2021_v200_", tile_name, "_Map.tif")
r <- terra::rast(url)

w <- terra::crop(r, terra::vect(l)) 

levels(w) <- n[c("map_code", "land_cover_class")] |>
  as.data.frame()
terra::coltab(w) <- n[c("map_code", "color")] |>
  as.data.frame()

terra::plot(w)

```

It's worth to mention that {geodata} package have `landcover()` function, which can download downsampled to 30-seconds per pixel rasters for specific land coverage class.
    
```{r}
#| label: geodata_landcover
#| eval: false

geodata::landcover(var = "moss", path = tempdir())
```


## Climate {#sec-climate}

Key climatic variables are:

* temperature (&deg;C) (minimum, maximum, average, above ground or at 2&nbsp;m height)
* precipitation (mm),
* solar radiation (kJ m^-2^ day^-1^),
* wind speed (m s^-1^),
* water vapor (cm liquid water) and water vapor pressure (kPa),
* probability of occurrence of snow, snow thickness (cm),
* soil moisture (cm liquid water),

Those data can be provided as point data sets (per station) or as gridded data sets obtained from analysis or modeling of the past and current climate data. 

### Stations data

#### Integrated Surface Database

The Integrated Surface Database (ISD) [@smithIntegratedSurfaceDatabase2011; @delgrecoSurfaceDataIntegration2006; @lottQualityControlIntegrated2004] is a global database that consists of hourly surface observations compiled from numerous sources into a single common ASCII format and common data model. It integrated data from numerous data sources and provides many parameters like temperature and dew point, wind speed and direction, precipitation etc. Data can be accessed from [ncei.noaa.gov](https://www.ncei.noaa.gov/products/land-based-station/integrated-surface-database) portal or directly from [server](https://www.ncei.noaa.gov/pub/data/noaa/) where the data is divided by year, and for every station a gziped text file is provided. Data format, described in @FederalClimateComplex2018, covers among others: date and time of observation, longitude, latitude and elevation of station, wind direction and speed, sky conditions, visibility distance, air temperature and dev point temperature, atmospheric pressure, precipitation and snow depth. Stations, coded like `040180-16201` can be translated to station names and their physical locations using [isd-history.csv](https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv) file.


```{r}
#| label: tbl-isd_stations
#| code-fold: true
#| tbl-cap: "A sample of ISD stations description file which can be used to translate station code to its location"

isd_stations <- read.csv("https://www.ncei.noaa.gov/pub/data/noaa/isd-history.csv")

isd_stations |>
  dplyr::slice_sample(n = 10) |>
  kableExtra::kable() |>
  kableExtra::kable_classic_2()

```

There is a {worldmet} [@worldmet] package which can be used to find the appropriate stations and import the data. Let's  use it to find a stations close to coordinates 15.5&deg;E, 51.5&deg;N and extract the data for one of them:

```{r}
#| label: worldmet_example
#| 
a <- worldmet::getMeta(lon = 15.5, lat = 51.5)

a[, c(1, 3, 6, 9:11, 15)]
```
By adding `returnMap = TRUE` parameter we can see the locations of stations returned:

```{r}
#| label: worldmet_return_map
#| echo: false

worldmet::getMeta(lon = 15.5, lat = 51.5, returnMap = TRUE)
```

Let's download a data for one station and see what's inside:

```{r}
#| label: worldmet_extract_data

station_data <- worldmet::importNOAA(code = "124240-99999",
                     year = 2022,
                     hourly = TRUE,
                     n.cores = 4,
                     path = "data")

station_data |>
  subset(select = c(3, 7:13)) |>
  tail()

```

Having data on hand we can perform analysis. A very simple wind rose and temperature diagram for WrocÅaw-Strachowice station is shown in @fig-station_data_examples.

```{r}
#| label: fig-station_data_examples
#| code-fold: true
#| fig-cap: "Examples of station's data vizualization: direction and wind speed (a) and temperature (b)"
#| fig-subcap: 
#|    - ""
#|    - ""
#| layout-ncol: 2

openair::windRose(station_data)

openair::timePlot(station_data, pollutant = "air_temp",
                  auto.text = FALSE,
                  ylab = "Temperature [\u00B0C]",
                  key = FALSE
                  )
```

#### Meteorological Aerodrome Report (METAR) data

You may have noticed in @tbl-isd_stations, some stations have ICAO (International Civil Aviation Organization) codes. It means, that such hourly data is provided by station located at airport. Copernicus airport at WrocÅaw-Strachowice is designated with `EPWR`. The data can be obtained from [Aviation Weather Center](https://aviationweather.gov/data/metar/), historical data is available from [ogimet](https://www.ogimet.com/index.phtml.en) (up to past 20 years) or [ASOS Network](https://mesonet.agron.iastate.edu/request/download.phtml)^[The Automated Surface Observing System]. In R we can use {pmetar} package [@pmetar]. 

```{r}
#| label: pmetar
#| message: false

library(pmetar)
epwr <- pmetar::metar_get(airport = "EPWR")
epwr
```

From the above report we can read: it was issued on `r pmetar::metar_day(epwr)` day current month at `r pmetar::metar_hour(epwr)` UTC. The wind was blowing from the direction of `r pmetar::metar_dir(epwr, numeric_only = TRUE)` degree and the speed was `r pmetar::metar_speed(epwr, metric = FALSE)` knots, etc, etc [@milradMETARCode2018].

```{r}
#| label: tbl-pmetar_EPWR
#| code-fold: true
#| message: false
#| tbl-cap: Decoded METARs for WrocÅaw-Strachowice airport

old <- pmetar::metar_get_historical(airport = "EPWR",
                             start_date = "2022-12-30",
                             end_date = "2022-12-31") |>
  pmetar::metar_decode() 

old |>
  dplyr::mutate(date = as.POSIXct(paste(METAR_Date, " ", Hour, ":00"))) |>
  subset(select = c(date, Wind_speed, Wind_direction, Temperature, Pressure, Dew_point), 
         date >= as.POSIXct("2022-12-31 18:00:00")) |>
  kableExtra::kable() |>
  kableExtra::kable_classic_2()
```

Comparing the data with ISD we can see the METAR reports are issued every 30 minutes.

### Gridded data

#### CHELSA

[CHELSA](https://chelsa-climate.org/) (Climatologies at High resolution for the Earthâs Land Surface Areas) is a data set hosted by the Swiss Federal Institute for Forest, Snow and Landscape Research WSL. The data is based on ERA-Interim climatic reanalysis and contains downscaled to 30 arc sec, ~&nbsp;1&nbsp;km global climate data. [@kargerClimatologiesHighResolution2017; @kargerClimatologiesHighResolution2021]. Ver. 2.1 of the data covers monthly datasets of temperatures and precipitation climatologies for years 1979--2019.

Files can be accessed directly from [Envicloud server](https://envicloud.wsl.ch/#/?prefix=chelsa%2Fchelsa_V2%2FGLOBAL%2F). The filename of each CHELSA data product follows a similar structure including the respective model used, the variable short name, the respective time variables, and the accumulation (or mean) period in the following basic format:

```
CHELSA_[short_name]_[timeperiod]_[Version].tif
```

For CMIP6 data: 

```
CHELSA_[short_name]_[timeperiod]_[model] _[ssp] _[Version].tif
```

Available variable names are listed in @tbl-chelsa_variable_names. TREELIM model and details (variables: `gsl`, `gsp ... fgd` was described by @paulsenClimatebasedModelPredict2014. KÃ¶ppen-Geiger `kg0` and `kg1` after @koeppenGeographischeSystemKlimate1936, `kg2` classification was updated by @peelUpdatedWorldMap2007, `kg3` according to @wissmannKlimaundVegetationsgebieteEurasiens1939, `kg4` --- new classification by @thornthwaiteClimatesNorthAmerica1931 and for details of `kg5` see [@trollKarteJahreszeitenKlimateErde1964]. Miami model for `npp` variable was described by @liethModelingPrimaryProductivity1975.

```{r}
#| label: tbl-chelsa_variable_names
#| echo: false
#| tbl-cap: "Chelsa variable names [@kargerCHELSAV2Technical2021]"
#| column: body-outset
#| output: markup

options(knitr.kable.NA = '')

tables <- rvest::read_html("https://chelsa-climate.org/bioclim/") |>
  rvest::html_table(header = TRUE)

t <- tables[[1]] |>
  dplyr::rowwise() |>
  dplyr::mutate(
    longname = tolower(longname),
    longname = stringr::str_replace(longname, pattern = "kÃ¶ppen-geiger", replacement = "KÃ¶ppen-Geiger"),
    unit = stringr::str_replace(unit, pattern = "Â°C/100", 
                                replacement = "Â°C"),
    unit = stringr::str_replace(unit, pattern = "m-2", 
                                replacement = "m<sup>-2</sup>"),
    unit = stringr::str_replace(unit, pattern = "mâ2", 
                                replacement = "m<sup>-2</sup>"),
    unit = stringr::str_replace(unit, pattern = "yrâ1", 
                                replacement = "yr<sup>-1</sup>"),
    scale = as.numeric(stringr::str_remove(scale, pattern = "â$")),
    offset = as.numeric(stringr::str_remove(offset, pattern = "â$")),
    explanation = paste0(toupper(substr(explanation, 1, 1)), substr(explanation, 2, nchar(explanation))),
    explanation = stringr::str_remove(explanation, "(https://doi.org/10.1007/s00035-014-0124-0)")
    )

if(!file.exists("data/chelsa_bioklim.rds")) {
  saveRDS(t, file = "data/chelsa_bioklim.rds")  
}

t |>
  kableExtra::kbl(
    col.names = c("Short name", "Long name", "Unit", "Scale", "Offset", "Explanation"),
    escape = FALSE) |>
  kableExtra::kable_classic_2()
  
```

Later the data was extended by new variables and got the name BIOCLIM+ [@brunGlobalClimaterelatedPredictors2022; @brunCHELSABIOCLIMNovelSet2022]. The variables provides:

- near-surface relative humidity (hurs)
- cloud area fraction (clt)
- near-surface wind speed (sfcWind)
- vapour pressure deficit (vpd)
- surface downwelling shortwave radiation (rsds)
- potential evapotranspiration (pet)
- climate moisture index (cmi)
- site water balance (swb)

Detailed description of the variables were provided by @kargerBIOCLIMNovelSet2022.

And finally, there is a [daily sets of data ](https://chelsa-climate.org/chelsa-w5e5-v1-0-daily-climate-data-at-1km-resolution/) [@kargerCHELSAW5E5V1W5E52022] which covers the entire globe (except the Arctic Ocean beyond 84&deg;N) at 30 arcsec horizontal and daily temporal resolution from 1979 to 2016. Variables (with short names and units in brackets) included in the CHELSA-W5E5 dataset are: Daily Mean Precipitation (pr) [kg m^-2^ s^-1^], Daily Mean Surface Downwelling Shortwave Radiation (rsds) [W m^-2^], Daily Mean Near-Surface Air Temperature (tas) [K], Daily Maximum Near Surface Air Temperature (tasmax) [K], Daily Minimum Near Surface Air Temperature (tasmin) [K], Surface Altitude (orog) [m], and the CHELSA-W5E5 land-sea mask (mask).

<!--
There are few R packages which helps to download and manage CHELSA data: {climatedata} [@climatedata], {chelsaDL} [@chelsaDL] and {ClimDatDownloadR} [@ClimDatDownloadR].
-->

To download CHELSA data in R you can use {ClimDatDownloadR} [@ClimDatDownloadR] or {climenv} [@climenv] packages.


#### WorldClim

[WorldClim](https://www.worldclim.org/data/index.html) is a database of high spatial resolution global weather and climate data [@fickWorldClimNew1km2017]. It contains monthly data: temperature (minimum, maximum and average), precipitation, solar radiation, vapour pressure and wind speed, aggregated across a target temporal range of 1970â2000. Historical climate data section covers 19 bioclimatic variables `BIO1...BIO19`. They are average for years 1970--2000. [Historical monthly weather data](https://www.worldclim.org/data/monthlywth.html) covers monthly weather data for years 1960--2021 downscaled from [@harrisVersionCRUTS2020]. The spatial resolution is 2.5 minutes (~21 km^2^ at the equator), 5 minutes or 10 minutes. Each `zip` file contains 120 GeoTiff (.tif) files, for each month of the year (January is 1; December is 12), for a 10 year period. The variables available are average minimum temperature [&deg;C], average maximum temperature [&deg;C] and total precipitation [mm].

To download data in R you can use `WorldClim.xxx` set of functions in {ClimDatDownloadR} package or `worldclim_xxx` and `cmip6_xxx` sets of functions in {geodata} package [@geodata]. There is `worldclim()` function in {climenv} package as well, it uses `geodata::worldclim_tile()` function for download.

#### ERA5 hourly data  

[ERA5](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels) is the fifth generation ECMWF reanalysis for the global climate and weather for the past 8 decades [@hersbachERA5GlobalReanalysis2020; @hersbachERA5HourlyData2023]. Data is available from 1940 onwards with hourly temporal resolution. Horizontal resolution is 0.25 x 0.25&deg; (atmosphere) and  0.5&deg; x 0.5&deg; (ocean waves). You can download it via [Copernicus portal](https://cds.climate.copernicus.eu/cdsapp#!/dataset/reanalysis-era5-single-levels?tab=form). There is a complementary data set named ERA5-Land [@munoz-sabaterERA5LandStateoftheartGlobal2021] covering time period from 1981 till today with spatial of 11 km and 1 hour temporal resolutions.

There are few packages which can be used to download the data: {KrigR} [@kuschKrigRToolDownloading2022] and {ecmwfr} [@ecmwfr; @ecmwfr2019]. In below example, using {ecmwfr}, we are going to check weather conditions in April and May of 1945 in Remagen, Germany.^[This is a response to [Open Data](https://opendata.stackexchange.com/questions/20999/weather-conditions-for-remagen-germany-in-spring-of-1945) question] After creating ECMWF account and setting up the key in keyring using `wf_set_key()`^[for details please check package documentation] function we can create data request and download it.

```{r}
#| label: remgen_bbox
#| echo: false
#| message: false

b <- osmdata::getbb("Remagen, Germany")

remagen_lon <- (b[1, 1] + b[1, 2])/2
remagen_lat <- (b[2, 1] + b[2, 2])/2

```


```{r}
#| label: ecmwfr_download
#| eval: false

b <- osmdata::getbb("Remagen, Germany")

remagen_lon <- (b[1, 1] + b[1, 2])/2
remagen_lat <- (b[2, 1] + b[2, 2])/2

options(keyring_backend="file")

request <- list(
  product_type = "reanalysis",
  variable = c("2m_temperature", "sea_surface_temperature", "total_precipitation"),
  year = "1945",
  month = c("04", "05"),
  day = c("01", "02", "03", "04", "05", "06", "07", "08", "09", "10", "11", "12", "13", "14", "15", "16", "17", "18", "19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31"),
  time = c("00:00", "01:00", "02:00", "03:00", "04:00", "05:00", "06:00", "07:00", "08:00", "09:00", "10:00", "11:00", "12:00", "13:00", "14:00", "15:00", "16:00", "17:00", "18:00", "19:00", "20:00", "21:00", "22:00", "23:00"),
  area = c(51, 7, 50, 8),
  format = "netcdf",
  dataset_short_name = "reanalysis-era5-single-levels",
  target = "download.nc"
)

nc <- ecmwfr::wf_request(
  user = "276798",
  time_out = 3600*10,
  request = request,   
  transfer = TRUE,  
  path = "data",
  verbose = TRUE
)
```

After executing such request you should see a message similar to below one:

```
Requesting data to the cds service with username 276798
- staging data transfer at url endpoint or request id:
  26df01ad-2c4f-4b70-a568-e58ba43e7deb

- timeout set to 10.0 hours
- polling server for a data transfer
Downloading file
  |============================================================================| 100%
- moved temporary file to -> data/download.nc
- Delete data from queue for url endpoint or request id:
  https://cds.climate.copernicus.eu/api/v2/tasks/26df01ad-2c4f-4b70-a568-e58ba43e7deb

```

Please note that preparing the data on server side and downloading it might take a long while. Having the data downloaded we can run the analysis. 

```{r}
#| label: fig-remagen_plot
#| code-fold: true
#| fig-cap: "Temperature in Remagen, Germany during April and May 1945"

K <- -273.15

m <- matrix(c(
  remagen_lon, remagen_lat),
  byrow = TRUE, ncol = 2,
  dimnames = list(c("rem"),
                  c("lon", "lat")))

p <- m |>
  terra::vect(m, type = "points", crs = "EPSG:4326")

nc <- "data/download.nc"
r <- terra::sds(nc)

temp2m <- terra::extract(r["t2m"], p, ID = TRUE, bind = TRUE) |>
  as.data.frame()
t_timestamp <- terra::time(r["t2m"])

f <- t(temp2m)
f <- f[3:nrow(f), ]
f <- f + K

f <- f |>
  as.data.frame()
f$t_timestamp <- t_timestamp

daily_means <- f |>
  dplyr::mutate(day = as.Date(t_timestamp)) |>
  dplyr::group_by(day) |>
  dplyr::summarise_at("f", mean) |>
  dplyr::mutate(day = as.POSIXct(paste(day, "00:00:00")))

plot(f$t_timestamp, f$f, type = "l",
     lty = 3,
     xlab = "1945",
     ylab = expression(paste("Temperature [",degree,"C]")),
     )
lines(daily_means$day, daily_means$f, lwd = 2, col = "blue")
legend("bottomright",
       legend = c("hourly", "daily mean"),
       col = c("black", "blue"),
       lty = c(3, 1),
       lwd = c(1, 2)
       )

```

```{r}
#| label: fig-remagen_precipitation
#| echo: false
#| fig-cap: "Precipitation in Remagen, Germany during April and May 1945"

m <- matrix(c(
  remagen_lon, remagen_lat),
  byrow = TRUE, ncol = 2,
  dimnames = list(c("rem"),
                  c("lon", "lat")))

p <- m |>
  terra::vect(m, type = "points", crs = "EPSG:4326")

nc <- "data/download.nc"
r <- terra::sds(nc)

t_timestamp <- terra::time(r["tp"])

tp <- terra::extract(r["tp"], p, ID = TRUE, bind = TRUE) |>
  as.data.frame()

f <- t(tp)
f <- f[3:nrow(f), ]
f <- f*1000

f <- f |>
  as.data.frame()
f$t_timestamp <- t_timestamp

daily_tp <- f |> 
  dplyr::mutate(day = as.Date(t_timestamp)) |>
  dplyr::group_by(day) |>
  dplyr::summarise_at("f", sum)

barplot(f ~ day, data = daily_tp,
        xlab = "1945",
        ylab = "Precipitation [mm]",
        axes = TRUE,
        names.arg = "")
axis(1,
     at = c(0, 36.1, nrow(daily_tp)+12.1),
     pos = 0,
     labels = c("Apr", "May", "Jun"),
     las = 1, lwd = 0, lwd.ticks = 1, line = -0.5, cex = 1.1)

```

#### TerraClimate

TerraClimate [@abatzoglouTerraClimateHighresolutionGlobal2018] is a dataset of monthly climate and climatic water balance for global terrestrial surfaces from 1958 to 2023. All data have monthly temporal resolution and a ~4-km (1/24th degree) spatial resolution. 

Primary climate variables covers: maximum and minimum temperature, vapor pressure, precipitation accumulation, downward surface shortwave radiation and wind-speed. Derived variables are: reference evapotranspiration (ASCE Penman-Montieth), runoff, actual evapotranspiration, climate water deficit, soil moisture, snow water equivalent, palmer drought severity index and vapor pressure deficit. Detailed description is provided on [Climatology Lab](https://www.climatologylab.org/terraclimate.html). The data is available as netCDF files from [THREDDS web server](http://thredds.northwestknowledge.net:8080/thredds/catalog/TERRACLIMATE_ALL/data/catalog.html). 

#### Global Maps from NASA Earth Observatory and other products

[Earth Observatory](https://earthobservatory.nasa.gov/global-maps) by NASA provides global monthly images at 10 km spatial resolution and covers years 2000-2022.

@maGlobalLongterm19812020 created global land surface temperature (LST) dataset, which was generated from NOAA advanced very-high-resolution radiometer (AVHRR) data (1981â2000) and includes three data layers: 

* instantaneous LST, a product generated by integrating several split-window algorithms with a random forest (RF-SWA)[@maGLASSLandSurface2020]; 

* orbital-drift-corrected (ODC) LST, a drift-corrected version of RF-SWA LST [@maGLASSLandSurface2020a];

* monthly averages of ODC LST [@maGLASSLandSurface2020b].

The resolution is 0.05&deg; spatially and 1-day temporal. In addition to Zenodo, the data is available at [http://glass.umd.edu/LST/](http://glass.umd.edu/LST/) in HDF files.

Based on ERA5 precipitation archive and MODIS monthly cloud cover frequency @kargerGlobalDailyKm2021 produced global daily 1km land surface [precipitation set](https://www.earthenv.org/precipitation) for the years 2003-2016. In 2021 @zhangGlobalSeamlessThinsp2022 published land surface temperature dataset for 2003-2020 with 1km spatial resolution and on daily (mid-daytime and mid-nightime) basis. The dataset is based on MODIS LST and spatiotemporal gap-filling framework. The data is available for download from [Iowa State University](https://doi.org/10.25380/iastate.c.5078492.v3) in GeoTIFF format. Another approach was taken by @yaoGlobalSeamlessHighresolution2023 who produced global seamless and high-resolution (30 arcsecond spatial) temperature (with land surface temperature (Ts) and near-surface air temperature (Ta)) dataset for 2001-2020. The data is available at [Middle Yangtze River Geoscience Date Center](https://cjgeodata.cug.edu.cn/#/pageDetail?id=97) (in Chinese).

<!---
hydro:
https://www.hydrosheds.org/hydrosheds-core-downloads
-->

<!--
soil:

https://data.isric.org/geonetwork/srv/eng/catalog.search#/home
-->


